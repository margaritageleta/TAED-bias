---
layout: post
title: "PHASE #2: Characterization and documentation of translation datasets"
description: "We have designed the generalized structure of a Datasheet that describes a corpus that has NOT been created by us (Section 1) and that is Machine Translation related (Section 2)."
---

# 2 Re-designing Datasheet for Datasets for when no corpus has been created

To be written!


# 3 Modifying Datasheet for Datasets for data used in Machine Translation

In the following, we list and explain several questions that could be added to the Datasheet for Datasets [1] when those are oriented to solve Machine Translation (MT) problems.

### Composition

It is known that MT Datasets don’t include rare or uncommon languages as many times as worldwide spoken languages, such as English. Not only are these datasets unbalanced in terms of languages, but they are also biased in terms of words. Hence, we should ask these question every time before creating a dataset to avoid more discriminance:
- **Does your dataset cover all possible languages equitatively?**
- **Is your data biased towards gender, ethics ...?**

It would also be good to consider the type of text contained in the data. This can also indirectly bias MT algorithms, for example in terms of representation of some words (such as contractions) or the presence of idioms.

- **Is the data made up of formal text, informal text or both equitably? Does it also contain slang terms?**

Furthermore, maybe the data is not completely made up of fully correct language and there’s the possibility of it containing, for example, extremely informal language to the point of being just not correct (hi, text messages!). This can be desirable for some applications, but non-desirable for others. In the case of MT, it could be useful to try and make the user avoid having to write perfectly in order to get quality translations in an environment where the user is not used to doing so.
- **Does the data contain incorrect language expressions on purpose?** If that’s the case, please provide which instances of the data correspond to them.

### Collection Process

The majority of data sources that are used to address Natural Language Processing (NLP) tasks, and more specifically MT tasks, consist of large corpora of plain text. We find it relevant to carefully think if any variant on source would give the same dataset as a result. Logically, as one gets more and more samples, the variances of the corpora extracted from different sources should tend to be smaller, but as an example, would one get the same descriptive adjectives for politicians from different newspapers (sources) ? We all know that it is completely false. Thus, we encourage dataset creators to carefully think about these biased sources.
- **If the same information was to be extracted from another source, would this information be similar?**
Moreover, and linked to the previous statement, it is possible that a corpus is made of data coming from different sources. In case the answer to the previous question is negative, then it is logical to ask the following question.
- **Does the data come from a single source or is it the result of a combination of data coming from different sources?** In case it is a compilation of different sources, please provide a link to those.


### Preprocessing

If any kind of bias or discriminance was detected in data, or the dataset creator is concerned about a potential detection of it, it is strongly recommended to mention any measures that have been applied to treat it.
-  **Was any mechanism applied to obtain more neutral language?**

It is possible that not all instances go through the same preprocessing pipeline, especially if the dataset is (partly) made of a compilation of several sources.
- **Were all instances preprocessed the same way?**

### Distribution
Users of MT applications can only take advantage if its language is considered by the application. Hence, we should care about covering as much as possible all regions with the language spoken.
- **Can all regions take the same profit from this dataset?**
- **Will the dataset be distributed worldwide, or only to areas where the considered languages are spoken widely?**


### Maintenance
It is well known that all languages evolve over time. Recent sources of plain text may use words following different distributions (trending topics, new vocabulary, and many other variants…). It’s important to think about the possibility of the implementation of an automatic process that takes care of these modifications.
- **Is vocabulary enrichment automatically developed?**


### References

[1] [Datasheets for Datasets](https://arxiv.org/pdf/1803.09010.pdf)
