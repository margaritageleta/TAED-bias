---
layout: post
title: "PHASE #2: Characterization and documentation of translation datasets"
description: "We have designed the generalized structure of a Datasheet that describes a corpus that has NOT been created by us (Section 1) and that is Machine Translation related (Section 2)."
permalink: TAED-bias/phase2/
---
In this task we are expected to design the generalized structure of a Datasheet that describes a corpus that has NOT been created by us (Section 1) and that is Machine Translation related (Section 2). In further tasks we will be creating a template for this and later on, filling it out with the information on the corpora we will be using.


# 2 Re-designing Datasheet for Datasets for when no corpus has been created

First, we will define those questions found in [Datasheets for Datasets](https://arxiv.org/pdf/1803.09010.pdf) that could be answered even when one is not the corpus creator. Next, we will state some more questions we have come up with that could be interesting to list.


### Motivation

First of all we need to go for the basics. The aim of this section is to explain why does the dataset exist.

- **Who created the dataset(e.g., which team, research group) and on behalf of which entity (e.g., company, institution, organization)?**

- **Did they fund it themselves? If there is an associated grant, please provide the name of the grantorand the grant name and number**

- **For what purpose was the data set created? Was there a specific task in mind?**


### Composition

Now that we know where is this data coming from, let's see what does it tell us about.

- **Are there multiple types of instances or is there just one type? Please specify the type(s) ( e.g. Raw data, preprocessed )**

- **What do the instances ( of each  type, if appropriate ) that comprise the data set represent (e.g., documents, photos, people, countries)?**
- **How many instances (of each type, if appropriate) are there in total?**

- **Does the dataset contain all possible instances or is it just a sample of a larger set?**

- **Is there a label or a target associated with each of the instances? If so, please provide a description.**

- **Is any information missing from individual instances? If so, please provide a description, explaining why this information is missing (e.g., because it was unavailable). This does not include intentionally removed information, but might include, e.g., redacted text.**

- **Are there any errors, sources of noise, or redundancies in the dataset? If so, please provide a description. Do not include missing information here.**

- **Are there recommended data splits (e.g., training, development/validation, testing)? If so, please provide a description of these splits, explaining the rationa lebehind them.**

- **Is the dataset self-contained, or does it link to or otherwise rely on external resources (e.g., websites, tweets, other datasets)? If it links to or relies on external resources, a) are there guarantees that they will exist, and remain constant, over time; b)are there ofﬁcial archival versions of the complete dataset (i.e., including the external resources as they existed at the time the dataset was created); c) are there any restrictions (e.g., licenses, fees) associated with any of the external resources that might apply to a future user? Please provide descriptions of all external resources and any restrictions associated with them, as well as links or other access points, as appropriate.**

- **Does the dataset contain data that might be considered conﬁdential (e.g.,data that is protected by legal privilegior by doctor patient conﬁdentiality, data that includes the content of individuals non-public communications)? If so, please provide a description.**

- **Does the dataset contain data that, if viewed directly, might be offensive, insulting, threatening, or might otherwise cause anxiety? If so, please describe why.**

- **Does the dataset relate to people? If so, please specify a) Whether the dataset identifies subpopulations or not. b) Wether the dataset identifies indivual people or not. c) Whether it contains information that could vulnerate any individuals or their rights. c) Any other comments.**

### Collection Process

Please specify any information regarding the collection progress that you may know (e.g. the person who created the dataset has explained ) or be able to find ( e.g. there exists and informational site about the dataset ). Please only include if verified.

### Preprocessing/Cleaning/Labeling

Please specify any information regarding the preprocessing that you may know (e.g. the person who created the dataset has explained ) or be able to find ( e.g. there exists and informational site about the dataset ). Please only include if verified.


### Uses

- **Has the dataset been used already?**

- **Is there repository that links to any or all papers or Systems that use data set?**

- **What other tasks couls the data set be used for? Please include your own intentions, if any.**

- **Are there tasks for which the data set should not be used?**

### Distribution

- **Please specify the source where you got the dataset from.**

- **Is the dataset distributed under a copyright or other intellectual property (IP) license, and/or under applicable terms of use (ToU)? Please cite a verified source.**


### Maintenance
- **Is there any verified manner of contacting the creator of the dataset?**

- **Is there any verified information on whether the dataset will be updatated in any form in the future?**

Below we list more questions we have come up with:

### Data selection

##### Regarding the source
- **Can we trust on that data?**

- **What is the origin of that data?** 

- **What is the quality of the source?**

##### Regarding the type of the file
- **What is the format of the data (.json, .xml, .csv, …)?** We need that to know how to extract the data.

##### Regarding the antiquity of the file
- **What is the antiquity of the file?** The behavior of the things changes with time.

### Data mining

A dataset can have an intended algorithm to be applied (a regression, a classification, a clustering algorithm, etc.). For that reason, we need to ask for additional parameters.

- **Are data in the dataset continuous or discrete?**

### Evaluation

##### Regarding the domain
- **In what units does the intented algorithm to be applied yield the result?**

# 3 Modifying Datasheet for Datasets for data used in Machine Translation

In the following, we list and explain several questions that could be added to the Datasheet for Datasets [1] when those are oriented to solve Machine Translation (MT) problems.

### Composition

It is known that MT Datasets don’t include rare or uncommon languages as many times as worldwide spoken languages, such as English. Not only are these datasets unbalanced in terms of languages, but they are also biased in terms of words. Hence, we should ask these question every time before creating a dataset to avoid more discriminance:
- **Does your dataset cover all possible languages equitatively?**
- **Is your data biased towards gender, ethics ...?**

It would also be good to consider the type of text contained in the data. This can also indirectly bias MT algorithms, for example in terms of representation of some words (such as contractions) or the presence of idioms.

- **Is the data made up of formal text, informal text or both equitably? Does it also contain slang terms?**

Furthermore, maybe the data is not completely made up of fully correct language and there’s the possibility of it containing, for example, extremely informal language to the point of being just not correct (hi, text messages!). This can be desirable for some applications, but non-desirable for others. In the case of MT, it could be useful to try and make the user avoid having to write perfectly in order to get quality translations in an environment where the user is not used to doing so.
- **Does the data contain incorrect language expressions on purpose?** If that’s the case, please provide which instances of the data correspond to them.

### Collection Process

The majority of data sources that are used to address Natural Language Processing (NLP) tasks, and more specifically MT tasks, consist of large corpora of plain text. We find it relevant to carefully think if any variant on source would give the same dataset as a result. Logically, as one gets more and more samples, the variances of the corpora extracted from different sources should tend to be smaller, but as an example, would one get the same descriptive adjectives for politicians from different newspapers (sources) ? We all know that it is completely false. Thus, we encourage dataset creators to carefully think about these biased sources.
- **If the same information was to be extracted from another source, would this information be similar?**
Moreover, and linked to the previous statement, it is possible that a corpus is made of data coming from different sources. In case the answer to the previous question is negative, then it is logical to ask the following question.
- **Does the data come from a single source or is it the result of a combination of data coming from different sources?** In case it is a compilation of different sources, please provide a link to those.


### Preprocessing

If any kind of bias or discriminance was detected in data, or the dataset creator is concerned about a potential detection of it, it is strongly recommended to mention any measures that have been applied to treat it.
-  **Was any mechanism applied to obtain more neutral language?**

It is possible that not all instances go through the same preprocessing pipeline, especially if the dataset is (partly) made of a compilation of several sources.
- **Were all instances preprocessed the same way?**

### Distribution
Users of MT applications can only take advantage if its language is considered by the application. Hence, we should care about covering as much as possible all regions with the language spoken.
- **Can all regions take the same profit from this dataset?**
- **Will the dataset be distributed worldwide, or only to areas where the considered languages are spoken widely?**


### Maintenance
It is well known that all languages evolve over time. Recent sources of plain text may use words following different distributions (trending topics, new vocabulary, and many other variants…). It’s important to think about the possibility of the implementation of an automatic process that takes care of these modifications.
- **Is vocabulary enrichment automatically developed?**


### References

[1] [Datasheets for Datasets](https://arxiv.org/pdf/1803.09010.pdf)
